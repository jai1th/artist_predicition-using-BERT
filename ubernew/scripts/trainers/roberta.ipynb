{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3971ce42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lyr_pred/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# train_vanilla.py\n",
    "import inspect\n",
    "import os, json, random\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7160d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # =======================\n",
    "# # USER CONFIG (edit this)\n",
    "# # =======================\n",
    "# @dataclass\n",
    "# class Config:\n",
    "#     data: str = \"/workspaces/artist_predicition-using-BERT/ubernew/data/lyrics_dataset.json\"          # JSON/JSONL with fields: text, label\n",
    "#     model: str = \"roberta-base\"                              # e.g. \"roberta-base\", \"distilbert-base-uncased\"\n",
    "#     out: str = \"results/roberta-base.vanilla\"                # fresh output dir\n",
    "#     batch: int = 16\n",
    "#     epochs: int = 4\n",
    "#     lr: float = 5e-5\n",
    "#     warmup_ratio: float = 0.06\n",
    "#     weight_decay: float = 0.01\n",
    "#     seed: int = 42\n",
    "#     val_ratio: float = 0.10                                  # from the remaining train split\n",
    "#     test_ratio: float = 0.10                                 # carved from full dataset\n",
    "#     max_length: int = 256\n",
    "#     logging_steps: int = 50\n",
    "#     num_workers: int = 2\n",
    "#     report_to_tb: bool = False                               # True → logs to TensorBoard\n",
    "\n",
    "# CONFIG = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9451e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    data: str = \"/workspaces/artist_predicition-using-BERT/ubernew/data/lyrics_dataset.json\"          # JSON/JSONL with fields: text, label\n",
    "    model: str = \"roberta-base\"\n",
    "    out: str = \"results/roberta-base.vanilla\"\n",
    "    batch: int = 16\n",
    "    epochs: int = 4\n",
    "    lr: float = 5e-5\n",
    "    warmup_ratio: float = 0.06\n",
    "    weight_decay: float = 0.01\n",
    "    seed: int = 42\n",
    "    val_ratio: float = 0.10\n",
    "    test_ratio: float = 0.10\n",
    "    max_length: int = 256\n",
    "    logging_steps: int = 50\n",
    "    num_workers: int = 2\n",
    "    report_to_tb: bool = False\n",
    "\n",
    "CONFIG = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27195e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, y_true = eval_pred\n",
    "    y_pred = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1_w, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    _, _, f1_m, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"f1_weighted\": f1_w, \"f1_macro\": f1_m, \"precision_w\": p, \"recall_w\": r}\n",
    "\n",
    "def main(cfg: Config):\n",
    "    seed_everything(cfg.seed)\n",
    "\n",
    "    # 1) Load dataset and encode labels -> ClassLabel (required for stratify)\n",
    "    raw = load_dataset(\"json\", data_files=cfg.data, split=\"train\")\n",
    "    raw = raw.class_encode_column(\"label\")  # turns string labels into ClassLabel-encoded ints\n",
    "\n",
    "    # 2) Split with stratification on the encoded \"label\" column\n",
    "    ds = raw.train_test_split(test_size=cfg.test_ratio, seed=cfg.seed, stratify_by_column=\"label\")\n",
    "    tmp = ds[\"train\"].train_test_split(test_size=cfg.val_ratio, seed=cfg.seed, stratify_by_column=\"label\")\n",
    "    dds = DatasetDict(train=tmp[\"train\"], validation=tmp[\"test\"], test=ds[\"test\"])\n",
    "\n",
    "    # 3) Get label names from feature metadata (avoids manual drift)\n",
    "    label_names = dds[\"train\"].features[\"label\"].names\n",
    "    id2label = {i: name for i, name in enumerate(label_names)}\n",
    "    label2id = {name: i for i, name in enumerate(label_names)}\n",
    "\n",
    "    # 4) Tokenizer & encode\n",
    "    tok = AutoTokenizer.from_pretrained(cfg.model, use_fast=True)\n",
    "\n",
    "    def preprocess(batch):\n",
    "        enc = tok(batch[\"text\"], truncation=True, max_length=cfg.max_length)\n",
    "        # batch[\"label\"] is already a list of ints from ClassLabel\n",
    "        enc[\"labels\"] = batch[\"label\"]\n",
    "        return enc\n",
    "\n",
    "    dds = dds.map(preprocess, batched=True, remove_columns=dds[\"train\"].column_names)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "    # 5) Fresh model (no resume)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg.model, num_labels=len(label_names), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "        # 6) Training args — force a clean run\n",
    "    def make_training_args(cfg):\n",
    "        base_kwargs = dict(\n",
    "            output_dir=cfg.out,\n",
    "            overwrite_output_dir=True,\n",
    "            learning_rate=cfg.lr,\n",
    "            per_device_train_batch_size=cfg.batch,\n",
    "            per_device_eval_batch_size=cfg.batch,\n",
    "            num_train_epochs=cfg.epochs,\n",
    "            weight_decay=cfg.weight_decay,\n",
    "            logging_steps=cfg.logging_steps,\n",
    "            seed=cfg.seed,\n",
    "            dataloader_num_workers=cfg.num_workers,\n",
    "            warmup_ratio=cfg.warmup_ratio,\n",
    "            save_total_limit=2,\n",
    "            report_to=([\"none\"] if getattr(cfg, \"report_to_tb\", False) is False else [\"tensorboard\"]),\n",
    "        )\n",
    "        sig = inspect.signature(TrainingArguments.__init__)\n",
    "        allowed = set(sig.parameters.keys())\n",
    "        safe_kwargs = {k: v for k, v in base_kwargs.items() if k in allowed}\n",
    "        return TrainingArguments(**safe_kwargs)\n",
    "\n",
    "    # ✅ actually create the args\n",
    "    training_args = make_training_args(cfg)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dds[\"train\"],\n",
    "        eval_dataset=dds[\"validation\"],\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 7) Evaluate + persist artifacts\n",
    "    os.makedirs(cfg.out, exist_ok=True)\n",
    "    metrics_val = trainer.evaluate(dds[\"validation\"])\n",
    "    metrics_test = trainer.evaluate(dds[\"test\"])\n",
    "    with open(os.path.join(cfg.out, \"metrics_val.json\"), \"w\") as f:\n",
    "        json.dump(metrics_val, f, indent=2)\n",
    "    with open(os.path.join(cfg.out, \"metrics_test.json\"), \"w\") as f:\n",
    "        json.dump(metrics_test, f, indent=2)\n",
    "    with open(os.path.join(cfg.out, \"labels.json\"), \"w\") as f:\n",
    "        json.dump({\"id2label\": id2label, \"label2id\": label2id}, f, indent=2)\n",
    "    with open(os.path.join(cfg.out, \"run_config.json\"), \"w\") as f:\n",
    "        json.dump(asdict(cfg), f, indent=2)\n",
    "\n",
    "    print(\"Saved:\", cfg.out)\n",
    "    print(\"Val:\", metrics_val)\n",
    "    print(\"Test:\", metrics_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8888fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 129/129 [00:00<00:00, 1577.33 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_11903/2244644724.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 67/264 16:22 < 49:37, 0.07 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.417800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(CONFIG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyr_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
